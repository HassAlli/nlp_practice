{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7VNe+uIr/IcD6+7p1CdMH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HassAlli/nlp_practice/blob/main/TextPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "1.   Cleaning\n",
        "2.   Tokenization\n",
        "3.   Stop words removel\n",
        "4.   Stemming\n",
        "5.   Lemmatization\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wUMGUUZbMvL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Cleaning**"
      ],
      "metadata": {
        "id": "rjuUa8TPRGUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "sent = \"Hello! This is Me! How are u?\"\n",
        "sent = re.sub(r'[^a-zA-Z\\s]', '', sent)\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TvA1Ax8ZO1rO",
        "outputId": "dc535d88-dfbe-4cda-a40f-69453802ea0c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello This is Me How are u'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tokenization**"
      ],
      "metadata": {
        "id": "YzT84vJ5RU2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "WB4oVq40PGml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"The quick brown fox jumps over the lazy dog. The dog barks loudly in response.\n",
        "Meanwhile, the cat watches from the top of the tree, unaffected by the commotion.\n",
        "The sun shines brightly on this beautiful day.\n",
        "Birds chirp merrily in the distance, adding to the serene atmosphere.\n",
        "Suddenly, a gust of wind rustles the leaves, creating a gentle whisper.\n",
        "Nature is alive with its own symphony of sounds.\n",
        "As evening approaches, the sky turns into a canvas of vibrant colors.\n",
        "The moon rises, casting a gentle glow over the landscape.\n",
        "It's a perfect time to unwind and reflect on the wonders of the world.\"\"\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "sentences"
      ],
      "metadata": {
        "id": "AtxygaF3QUyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Removing Stop words**"
      ],
      "metadata": {
        "id": "w6zWfWQCTDXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "IbRiH3BhSWtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Stemming**"
      ],
      "metadata": {
        "id": "2KCxOnCRULg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "words = nltk.word_tokenize(paragraph)\n",
        "\n",
        "for word in words:\n",
        "  print(stemmer.stem(word))"
      ],
      "metadata": {
        "id": "DoFYlZC6TadL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Lemmatization**"
      ],
      "metadata": {
        "id": "YNgXBYSJVMyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = nltk.word_tokenize(paragraph)\n",
        "for word in words:\n",
        "  print(lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "id": "PTirmki0VBeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying the preprecessing**"
      ],
      "metadata": {
        "id": "CPd8_LbhWOlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk import stem\n",
        "# from nltk import PorterStemmer\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "for sent in sentences:\n",
        "  print(sent)\n",
        "\n",
        "# stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# stemming\n",
        "for i in range(len(sentences)):\n",
        "  sentences[i] = re.sub(r'[^a-zA-Z\\s]', '', sentences[i])\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)\n",
        "\n",
        "print(\"_________________________________\\n\")\n",
        "for sent in sentences:\n",
        "  print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkcBrVYRVr4F",
        "outputId": "646a1a5e-e3b9-4e16-eae7-2c7bd06b1f70"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox jumps over the lazy dog.\n",
            "The dog barks loudly in response.\n",
            "Meanwhile, the cat watches from the top of the tree, unaffected by the commotion.\n",
            "The sun shines brightly on this beautiful day.\n",
            "Birds chirp merrily in the distance, adding to the serene atmosphere.\n",
            "Suddenly, a gust of wind rustles the leaves, creating a gentle whisper.\n",
            "Nature is alive with its own symphony of sounds.\n",
            "As evening approaches, the sky turns into a canvas of vibrant colors.\n",
            "The moon rises, casting a gentle glow over the landscape.\n",
            "It's a perfect time to unwind and reflect on the wonders of the world.\n",
            "_________________________________\n",
            "\n",
            "The quick brown fox jump lazy dog\n",
            "The dog bark loudly response\n",
            "Meanwhile cat watch top tree unaffected commotion\n",
            "The sun shine brightly beautiful day\n",
            "Birds chirp merrily distance adding serene atmosphere\n",
            "Suddenly gust wind rustle leaf creating gentle whisper\n",
            "Nature alive symphony sound\n",
            "As evening approach sky turn canvas vibrant color\n",
            "The moon rise casting gentle glow landscape\n",
            "Its perfect time unwind reflect wonder world\n"
          ]
        }
      ]
    }
  ]
}